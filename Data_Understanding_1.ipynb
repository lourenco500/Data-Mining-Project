{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2fb5ad",
   "metadata": {},
   "source": [
    "# Data Mining Project - Group XX 2025/2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c97d4",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e437cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import ceil\n",
    "\n",
    "from itertools import product\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# for better resolution plots\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#o svg consegue ampliar infinitamente os gráficos sem perder qualidade mas às vezes é mais lento \n",
    "#por isso agora usamos retina\n",
    "\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aed8a5",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a35e2",
   "metadata": {},
   "source": [
    "Import the datasets from csv files using commas as separators of the columns and setting the unique customer identifier as the index of both columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDB = pd.read_csv('DM_AIAI_FlightsDB.csv', sep = \",\", index_col= \"Loyalty#\")\n",
    "customerDB = pd.read_csv('DM_AIAI_CustomerDB.csv', sep = \",\", index_col= \"Loyalty#\")\n",
    "metaData = pd.read_csv('DM_AIAI_Metadata.csv', sep = \";\", header= None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b6f94",
   "metadata": {},
   "source": [
    "Remove the 'Unnamed' column referring to a sequential numbering of the rows, as we set the column \"Loyalty#\" as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da13cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB = customerDB.iloc[:, 1:]\n",
    "customerDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb809032",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e789c70",
   "metadata": {},
   "source": [
    "**FlightsDB Database Variable Description**\n",
    "- **Loyalty#:**\tUnique customer identifier linking to CustomerDB\n",
    "- **Year:**\tYear of flight activity record\n",
    "- **Month:**\tMonth of flight activity record (1-12)\n",
    "- **YearMonthDate:**\tFirst day of the month for the activity period\n",
    "- **NumFlights:**\tTotal number of flights taken by customer in the month\n",
    "- **NumFlightsWithCompanions:**\tNumber of flights where customer traveled with companions\n",
    "- **DistanceKM:**\tTotal distance traveled in kilometers for the month\n",
    "- **PointsAccumulated:**\tLoyalty points earned by customer during the month\n",
    "- **PointsRedeemed:**\tLoyalty points spent/redeemed by customer during the month\n",
    "- **DollarCostPointsRedeemed:**\tDollar value of points redeemed during the month\n",
    "\n",
    "**CustomerDB Database Variable Description**\n",
    "- **Loyalty#:**  Unique customer identifier for loyalty program members\n",
    "- **First Name:**   Customer's first name\n",
    "- **Last Name:**   Customer's last name \n",
    "- **Customer Name:** Customer's full name (concatenated)\n",
    "- **Country:**\tCustomer's country of residence\n",
    "- **Province or State:**\tCustomer's province or state\n",
    "- **City:**\tCustomer's city of residence\n",
    "- **Latitude:**\tGeographic latitude coordinate of customer location\n",
    "- **Longitude:**\tGeographic longitude coordinate of customer locatio\n",
    "- **Postal code:**\tCustomer's postal/ZIP code\n",
    "- **Gender:**\tCustomer's gender\n",
    "- **Education:**\tCustomer's highest education level (Bachelor, College, etc.)\n",
    "- **Location:** Code\tUrban/Suburban/Rural classification of customer residence\n",
    "- **Income:**\tCustomer's annual income\n",
    "- **Marital Status:**\tCustomer's marital status (Married, Single, Divorced)\n",
    "- **LoyaltyStatus:**\tCurrent tier status in loyalty program (Star > Nova > Aurora)\n",
    "- **EnrollmentDateOpening:**\tDate when customer joined the loyalty program\n",
    "- **CancellationDate:**\tDate when customer left the program\n",
    "- **Customer Lifetime:** Value\tTotal calculated monetary value of customer relationship\n",
    "- **EnrollmentType:**\tMethod of joining loyalty program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c5569d",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f03b5e",
   "metadata": {},
   "source": [
    "Define the project's objectives and requirements by translating business goals into data science goals. \n",
    "This involves understanding the business problem, identifying success criteria, determining resource needs, and creating an initial project plan with stages, duration, and costs.\n",
    "\n",
    "Business Success criteria: \n",
    "- “A 5% reduction in churn results in €50k monthly savings.”\n",
    "\n",
    "Data mining Success criteria: \n",
    "- “Model accuracy ≥ 85% on test data.” \n",
    "- “Segments must be interpretable and actionable by marketing.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b037728d",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee0e54",
   "metadata": {},
   "source": [
    "On this section we will inspect the data shape, column names and data types for each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c4971",
   "metadata": {},
   "source": [
    "## General Look at the DataSet (FlightsDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ec954",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDB.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c0347",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDB.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e78c01",
   "metadata": {},
   "source": [
    "From the visualization of the head and tail of the data base we can already understand that some errors exist:\n",
    "\n",
    "    - NumFlights and NumFlightsWithCompanions as floats...\n",
    "    - PointsAccumulated and PointsRedeemed as floats. Should they be integers?\n",
    "We will further analyse this using describe and info.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17515166",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDB.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b848d34",
   "metadata": {},
   "source": [
    "From info we can see that:\n",
    "\n",
    "    - NumFlights and NumFlightsWithCompanions as floats...\n",
    "    - PointsAccumulated and PointsRedeemed as floats. Should they be integers? \n",
    "    - There aren't missing values\n",
    "\n",
    "What will we do?\n",
    "\n",
    "    Analyse with describe to have a different view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f904783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To confirm that missing values don't exist\n",
    "flightsDB.replace(\"\", np.nan, inplace=True)\n",
    "flightsDB.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDB.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ebc58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDB.describe(include='object')\n",
    "\n",
    "#o \"top\" é a moda e \"freq\" é a frequencia do valor mais frequente\n",
    "#\"unique\" é a quantidade de valores unicos ((36 datas diferentes pq é o primeiro dia de cada mês durante 3 anos))\n",
    "#\"count\" é o numero de valores nao nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d5a0e",
   "metadata": {},
   "source": [
    "From both numeric and categorical describe we don't notice any weird value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c958a90",
   "metadata": {},
   "source": [
    "## Data Exploration and Analysis (FlightDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858810cf",
   "metadata": {},
   "source": [
    "### Unique, Max, Min "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34328d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flightsDB[\"Year\"].unique())\n",
    "print(flightsDB[\"Month\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5171cfa4",
   "metadata": {},
   "source": [
    "From the code above we can see that our dataset have only values from the years of 2019, 2020 and 2021 and have values from all months of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1fee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(flightsDB[\"NumFlights\"].max(), flightsDB[\"NumFlights\"].min())\n",
    "#from this we can see that there are some customers with 0 flights in a month and the maximum number of flights is 21 in a month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e31971",
   "metadata": {},
   "source": [
    "### Values Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c2b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDB[\"NumFlights\"].value_counts()\n",
    "#it looks like the most common number of flights in a month is 0, meaning that many customers don't fly every month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc489c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDB[\"NumFlightsWithCompanions\"].value_counts()\n",
    "#similarly to NumFlights, the most common value is 0 but the maximum number of flights with companions is 9.9 (float?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b4554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flightsDB[\"Year\"].value_counts())\n",
    "#we can see that the number of records for each year is equally distributed\n",
    "\n",
    "print('-------------------------------------')\n",
    "\n",
    "print(flightsDB[\"Month\"].value_counts())\n",
    "#we can see that the number of records for each month is equally distributed just like for the years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac5f208",
   "metadata": {},
   "source": [
    "### Check Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174feff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how many duplicates exist\n",
    "print(flightsDB.duplicated().sum())\n",
    "\n",
    "#check the percentage of duplicates in our DataFrame\n",
    "print(flightsDB.duplicated().sum() / len(flightsDB) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba3cd0",
   "metadata": {},
   "source": [
    "!!!!    The percentage of duplicates ir almost 50%    !!!!\n",
    "\n",
    "Because of this we understand that having Loyalty# as an index can be a wrong approach to check the duplicates so we read again our csv file and assign it to the variable flightsDB with the Loyalty# as a feature to check again the duplicates considering this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a40a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDB = pd.read_csv('DM_AIAI_FlightsDB.csv', sep = \",\")\n",
    "flightsDB.duplicated().sum() / len(flightsDB) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ff2c0",
   "metadata": {},
   "source": [
    "From the new calculation we obtain only 0.48% of duplicated which it makes more sense in our problem.\n",
    "\n",
    "With this value we can decide to drop the duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28bcdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDB[flightsDB[\"Loyalty#\"] == 263267]\n",
    "#Here we check that there are duplicates for the Loyalty# number 263267\n",
    "#the DataFrame below show us all the Data associated to this Loyalty number and we can see that some rows have the exactly same information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d6447d",
   "metadata": {},
   "source": [
    "There are 72 equal rows meaning all 36 unique values (corresponding to 12 months over 3 years) are duplicated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8adecf",
   "metadata": {},
   "source": [
    "As said before, to be sure that we are not losing any information, we need to introduce the column \"Loyalty#\" as a feature and not a index. Because of that the code that follows assign the variable FlightsDB to the new variable created that consider \"Loyalty#\" as a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63f8c02",
   "metadata": {},
   "source": [
    "After all the reasoning about the duplicates we decide to drop the duplicates, since they represent a minimal percentage of the total data and such a loss of information will not be significant for the final objective of this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16562a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we drop the duplicates from the DataFrame with index\n",
    "flightsDB.drop_duplicates(inplace= True)\n",
    "\n",
    "# Check that the duplicates were removed\n",
    "flightsDB.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b97b1c",
   "metadata": {},
   "source": [
    "### New Values Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a169aee7",
   "metadata": {},
   "source": [
    "After dropping the duplicates we think that's important to verify again the values of each year and month that were to well distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59460769",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flightsDB[\"Year\"].value_counts())\n",
    "\n",
    "print('--------------------------------')\n",
    "\n",
    "print(flightsDB[\"Month\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c983e",
   "metadata": {},
   "source": [
    "It's possible to understand that the values changed but they are still quite similar. It's obvious that the same will happen if we count the values for the NumFlights and NumFlightWithCompanions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63722f2a",
   "metadata": {},
   "source": [
    "### Correlation between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c774e4ce",
   "metadata": {},
   "source": [
    "This correlation is also an important analysis to be done. However this doesn't make sense for all variables so we create a new DataFrame with only the variables we want to use to check the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a756d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = flightsDB[[\"Year\", \"Month\", \"NumFlights\", \"NumFlightsWithCompanions\", \"DistanceKM\", \"PointsAccumulated\", \"PointsRedeemed\", \"DollarCostPointsRedeemed\"]]\n",
    "\n",
    "new.corr(method=\"pearson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c2149",
   "metadata": {},
   "source": [
    "From the code before it's difficult to get conclusions. We will visualize this matrix in a easy way of getting conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = new.corr(method=\"pearson\"). round(2)\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,                # hide upper triangle\n",
    "    annot=True,               # show values\n",
    "    cmap=\"coolwarm\",          # divergent color map\n",
    "    center=0,                 # center colormap in 0\n",
    "    linewidths=0.5,           # lines between cells to help visualization\n",
    "    vmin=-1, vmax=1,          # fix scale\n",
    "    square=True               # make cells square-shaped\n",
    ")\n",
    "\n",
    "\n",
    "plt.title(\"Correlation Matrix (Pearson)\", fontsize=14, pad=15)\n",
    "plt.tight_layout() # improve layout by reducing overlaps\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a008202",
   "metadata": {},
   "source": [
    "Now the analysis of the correlation between each two variables it's much more easy. With this we understand that some variables are perfectly correlated, what let us think that maybe we should not consider all variables to go on with the work. \n",
    "\n",
    "PointsRedeemed and DollarCostPointsRedeemed, DistanceKm and Points Accumulated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c3ec6",
   "metadata": {},
   "source": [
    "## General Look at the Data (CustomerDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ffc3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e22bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afc7907",
   "metadata": {},
   "source": [
    "From the visualization of the head and tail of the data base we can already understand that some errors exist:\n",
    "\n",
    "    - Missing values in some features\n",
    "    - EnrollmentType as \"2021 Promotion\" when it's suppose to be a type\n",
    "We will further analyse this using describe and info.\n",
    "\n",
    "It's also possible to see that some variables are redundante, such as Costumer Name, First Name and Last Name\n",
    "To solve this problem we will uniformize all the values in data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ce71e",
   "metadata": {},
   "source": [
    "From info we can see that:\n",
    "\n",
    "    - missing values in Income, CustomerLifetimeValue, CancellationDate\n",
    "* the missing values in the features Income can make sense in cases where customers do not want to share their personal annual income. Or they may also be input errors. (Depends on interpretation).\n",
    "\n",
    "* We can also believe that it makes sense to have NaN values in “CancellationDate,” as this means that there are customers who have not left the program.\n",
    "\n",
    "* For the “CustomerLifetimeValue” variable, we believe that it does not make sense to have NaN values because even if the customer has no value for the company, their CustomerLifetimeValue will be 0.\n",
    "\n",
    "What will we do?\n",
    "\n",
    "    Analyse with describe to have a different view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To confirm that missing values exist\n",
    "customerDB.replace(\"\", np.nan, inplace=True)\n",
    "customerDB.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66541fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03809f18",
   "metadata": {},
   "source": [
    "From the numerical describe we can see that:\n",
    "\n",
    "    - Once again we have the column Unnamed that has no relevant values\n",
    "\n",
    "From the rest of the infromation we can't find any other problem from the first look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69951f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41b9e5f",
   "metadata": {},
   "source": [
    "From the object describe we can conclude that:\n",
    "\n",
    "    - there are no repeted Customer Names (count = unique = 16921);\n",
    "    - there's only one Country, Canada\n",
    "    - other things that will be analysed latter if they are relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1062e12e",
   "metadata": {},
   "source": [
    "## Data Exploration and Analysis (CustomerDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3573b840",
   "metadata": {},
   "source": [
    "### Unique, Max, Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ca879",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customerDB[\"Country\"].unique()) # with this we can see that only one country exists in the data base\n",
    "print('-------------------------------------')\n",
    "print(customerDB[\"Education\"].unique())\n",
    "print('-------------------------------------')\n",
    "print(customerDB[\"Location Code\"].unique())\n",
    "print('-------------------------------------')\n",
    "print(customerDB[\"Marital Status\"].unique())\n",
    "print('-------------------------------------')\n",
    "print(customerDB[\"LoyaltyStatus\"].unique())\n",
    "print('-------------------------------------')\n",
    "print(customerDB[\"EnrollmentType\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf549d37",
   "metadata": {},
   "source": [
    "From the results above we can see there aren't weird values for the features analysed. We can also verify that all Costumer's reside in Canada but in different areas, because there's diffrent Location Codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af553c0",
   "metadata": {},
   "source": [
    "### Values Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customerDB[\"Postal code\"].value_counts()) \n",
    "#check the frequency of each postal code and we notice that some postal codes are much more common than others\n",
    "\n",
    "print('-------------------------------------')\n",
    "print(customerDB[\"Gender\"].value_counts()) \n",
    "#we conclude that man and woman customers are almost equally represented in the data base\n",
    "\n",
    "print('-------------------------------------')\n",
    "print(customerDB[\"Education\"].value_counts()) \n",
    "#we can see that most customers have a Bachelor degree and few have a Master's\n",
    "\n",
    "print('-------------------------------------')\n",
    "print(customerDB[\"Location Code\"].value_counts()) \n",
    "#the location codes are quite equally distributed\n",
    "\n",
    "print('-------------------------------------')\n",
    "print(customerDB[\"Marital Status\"].value_counts()) \n",
    "#most customers are married and only a few are divorced\n",
    "\n",
    "print('-------------------------------------')\n",
    "print(customerDB[\"LoyaltyStatus\"].value_counts()) \n",
    "#there are way more Gold members and platinum members are the least common\n",
    "\n",
    "print('-------------------------------------')\n",
    "print(customerDB[\"EnrollmentType\"].value_counts()) \n",
    "# most customers enrolled through a promotion and very few through 2021 promotion, the difference is huge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f4716",
   "metadata": {},
   "source": [
    "#### Check Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699449b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1921f027",
   "metadata": {},
   "source": [
    "Checking the duplicates we verify that we don´t have any.\n",
    "\n",
    "But it's still important to check the duplicates without the names features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB_no_name = customerDB.drop(columns=[\"First Name\", \"Last Name\", \"Customer Name\"])\n",
    "customerDB_no_name.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d3f2cf",
   "metadata": {},
   "source": [
    "The result is the same so we can conclude that there aren't duplicated values in this DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522bb58",
   "metadata": {},
   "source": [
    "Contrary to what we have seen with the Flights dataset, here is not important to consider the values of Loyalty# as a feature. Still, so we can be consistint when analysing our datasets, we will had this column as a feature also to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c05e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB = pd.read_csv('DM_AIAI_CustomerDB.csv', sep = \",\")\n",
    "# code that we also did in the begining because there's a column with the index numbers that is completely unuseful\n",
    "customerDB = customerDB.iloc[:, 1:] \n",
    "\n",
    "# to verify that the Loyalty# is now a feature and not an index anymore\n",
    "customerDB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a8f824",
   "metadata": {},
   "source": [
    "### Correlation between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82edb296",
   "metadata": {},
   "source": [
    "As before this correlation is also an important analysis to be done. However this doesn't make sense for all variables so we create a new DataFrame with only the variables we want to use to check the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabda146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the customerDB and select only the relevant columns for correlation analysis\n",
    "new = customerDB.copy()\n",
    "new = new[[\"Latitude\", \"Longitude\", \"Income\", \"Customer Lifetime Value\", \"EnrollmentDateOpening\", \"CancellationDate\"]]\n",
    "\n",
    "# converting date columns to datetime format\n",
    "new['EnrollmentDateOpening'] = pd.to_datetime(new['EnrollmentDateOpening'], format='%m/%d/%Y', errors='coerce')\n",
    "new['CancellationDate'] = pd.to_datetime(new['CancellationDate'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# using the two date columns converted before to create a new column with the customer duration in days\n",
    "new['CustomerDurationDays'] = (new['CancellationDate'] - new['EnrollmentDateOpening']).dt.days\n",
    "\n",
    "# choose the numerical columns for correlation analysis\n",
    "cols = [\"Latitude\", \"Longitude\", \"Income\", \"Customer Lifetime Value\", \"CustomerDurationDays\"]\n",
    "new[cols].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b0cec6",
   "metadata": {},
   "source": [
    "From the code before it's difficult to get conclusions. We will visualize this matrix in a easy way of getting conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = new.corr(method=\"pearson\"). round(2)\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,                # hide upper triangle\n",
    "    annot=True,               # show values\n",
    "    cmap=\"coolwarm\",          # divergent color map\n",
    "    center=0,                 # center colormap in 0\n",
    "    linewidths=0.5,           # lines between cells to help visualization\n",
    "    vmin=-1, vmax=1,          # fix scale\n",
    "    square=True               # make cells square-shaped\n",
    ")\n",
    "\n",
    "\n",
    "plt.title(\"Correlation Matrix (Pearson)\", fontsize=14, pad=15)\n",
    "plt.tight_layout() # improve layout by reducing overlaps\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6268a399",
   "metadata": {},
   "source": [
    "Now the analysis of the correlation between each two variables it's much more easy. With this we understand that the variables EnrollmentDateOpening and CustomerDurationDays are correlated, but we don't think that this is a value that lead us to drop one of this variables. The same happen for the variables Latitude and Longitude that have a bigger correlation but maybe not enough to drop one of this variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd2156",
   "metadata": {},
   "source": [
    "## Data Quality Check in both Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b7d287",
   "metadata": {},
   "source": [
    "Identificar missing values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fall2526",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
